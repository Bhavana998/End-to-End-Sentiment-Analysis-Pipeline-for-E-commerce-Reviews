# -*- coding: utf-8 -*-
"""Setty bhavana - Data Scientist - Round 1.ipynb

Automatically generated by Colab.

Original file is located at
https://colab.research.google.com/drive/12rJRI63KWDRNlItQI9nIRcD7IqmDcKf6?usp=sharing

# ***Load both datasets***

# ***“We imported two datasets of Product A (iPhone 14) and Product B (Nike Shoes) each containing ~600 reviews in CSV format.”***
"""

import pandas as pd

df1 = pd.read_csv("/content/iphone14_customer_review.csv")
df2 = pd.read_csv("/content/web_scraped.csv")

df1.head()

df2.head()

"""# **Basic Cleaning**"""

def clean_basic(df, review_column_name):
    df = df.drop_duplicates()
    df = df.dropna(subset=[review_column_name])
    return df

df1 = clean_basic(df1, 'review')
df2 = clean_basic(df2, 'Review')

"""# **Create Binary Sentiment Labels**"""

def create_labels(df, rating_column_name):
    # Filter out neutral ratings (assuming 3 is neutral) and create a copy to avoid SettingWithCopyWarning
    df = df[df[rating_column_name] != 3].copy()
    # Create binary sentiment: 1 for positive (rating >= 4), 0 for negative (rating <= 2)
    df['sentiment'] = df[rating_column_name].apply(lambda x: 1 if x >= 4 else 0)
    return df

df1 = create_labels(df1, 'rating')
df2 = create_labels(df2, 'Star_rating')

"""# **Text Preprocessing**"""

import re

def preprocess(text):
    text = str(text).lower()
    text = re.sub(r"http\S+", "", text)
    text = re.sub(r"[^a-z0-9\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

df1['clean_text'] = df1['review'].apply(preprocess)
df2['clean_text'] = df2['Review'].apply(preprocess)

"""# **Exploratory Data Analysis (EDA)**

# ***1. Sentiment distribution***
"""

import matplotlib.pyplot as plt

df1['sentiment'].value_counts().plot(kind='pie', autopct='%1.1f%%')
plt.title("iPhone Sentiment Distribution")
plt.show()

plt.figure(figsize=(5,5))
df2['sentiment'].value_counts().plot(kind='pie', autopct='%1.1f%%')
plt.title("Nike Shoes Sentiment Distribution")
plt.ylabel("")
plt.show()

"""***2. Rating distribution***"""

df1['rating'].value_counts().plot(kind='bar')
plt.title("iPhone Rating Histogram")
plt.xlabel("Rating")
plt.ylabel("Count")
plt.show()

plt.figure(figsize=(6,4))
df2['Star_rating'].value_counts().sort_index().plot(kind='bar')
plt.title("Nike Shoes Rating Distribution")
plt.xlabel("Rating")
plt.ylabel("Count")
plt.show()

"""***3. Wordcloud***"""

from wordcloud import WordCloud

# iPhone
wc1 = WordCloud(width=800, height=400, background_color='white').generate(
    " ".join(df1['clean_text'])
)

plt.figure(figsize=(10,5))
plt.imshow(wc1)
plt.axis("off")
plt.title("iPhone - WordCloud")
plt.show()

# Nike Shoes
wc2 = WordCloud(width=800, height=400, background_color='white').generate(
    " ".join(df2['clean_text'])
)

plt.figure(figsize=(10,5))
plt.imshow(wc2)
plt.axis("off")
plt.title("Nike Shoes - WordCloud")
plt.show()

"""**Monthly Sentiment Trend**"""

df1['review_date'] = pd.to_datetime(df1['dates'], format='%b, %Y', errors='coerce')
df2['review_date'] = pd.to_datetime(df2['Date'], format='%d/%m/%Y', errors='coerce')

# iPhone monthly sentiment
df1.set_index('review_date').resample('M')['sentiment'].mean().plot(figsize=(8,4))
plt.title("iPhone Monthly Average Sentiment")
plt.ylabel("Sentiment Score (0–1)")
plt.show()

# Nike monthly sentiment
df2.set_index('review_date').resample('M')['sentiment'].mean().plot(figsize=(8,4))
plt.title("Nike Shoes Monthly Average Sentiment")
plt.ylabel("Sentiment Score (0–1)")
plt.show()

"""**Most Common Positive & Negative Words**"""

from sklearn.feature_extraction.text import CountVectorizer

pos_text = df1[df1['sentiment']==1]['clean_text']
neg_text = df1[df1['sentiment']==0]['clean_text']

cv = CountVectorizer(stop_words='english', max_features=20)

# Positive
pos_cv = cv.fit_transform(pos_text)
pos_freq = dict(zip(cv.get_feature_names_out(), pos_cv.toarray().sum(axis=0)))
print("Most Common Positive Words (iPhone):")
print(pos_freq)

# Negative
cv = CountVectorizer(stop_words='english', max_features=20)
neg_cv = cv.fit_transform(neg_text)
neg_freq = dict(zip(cv.get_feature_names_out(), neg_cv.toarray().sum(axis=0)))
print("Most Common Negative Words (iPhone):")
print(neg_freq)

"""**Word Frequency Bar Plot**"""

from collections import Counter

word_counts = Counter(" ".join(df1['clean_text']).split()).most_common(15)

words = [x[0] for x in word_counts]
freqs = [x[1] for x in word_counts]

plt.figure(figsize=(8,4))
plt.bar(words, freqs)
plt.xticks(rotation=45)
plt.title("iPhone Top 15 Frequent Words")
plt.show()

word_counts = Counter(" ".join(df2['clean_text']).split()).most_common(15)

words = [x[0] for x in word_counts]
freqs = [x[1] for x in word_counts]

plt.figure(figsize=(8,4))
plt.bar(words, freqs)
plt.xticks(rotation=45)
plt.title("Nike Shoes Top 15 Frequent Words")
plt.show()

"""**TRAIN–TEST SPLIT**"""

from sklearn.model_selection import train_test_split

X = df1['clean_text']
y = df1['sentiment']

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print("Train size:", len(X_train))
print("Test size:", len(X_test))

"""**TF-IDF VECTORIZATION**"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(
    max_features=10000,  # choose top 10k words
    ngram_range=(1,2),   # unigrams + bigrams
    stop_words='english'
)

X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

X_train_tfidf.shape, X_test_tfidf.shape

"""**MODEL TRAINING**"""

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import ComplementNB
from sklearn.ensemble import RandomForestClassifier

models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Naive Bayes": ComplementNB(),
    "Random Forest": RandomForestClassifier(n_estimators=200)
}

for name, model in models.items():
    print("Training:", name)
    model.fit(X_train_tfidf, y_train)
print("All models trained successfully!")

"""**MODEL EVALUATION**"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

for name, model in models.items():
    pred = model.predict(X_test_tfidf)

    print("------------", name, "------------")
    print("Accuracy:", accuracy_score(y_test, pred))
    print("Precision:", precision_score(y_test, pred))
    print("Recall:", recall_score(y_test, pred))
    print("F1 Score:", f1_score(y_test, pred))
    print("Confusion Matrix:\n", confusion_matrix(y_test, pred))
    print("---------------------------------------\n")

"""**CONFUSION MATRIX HEATMAP**"""

import seaborn as sns
import matplotlib.pyplot as plt

best_model = models["Logistic Regression"]
y_pred = best_model.predict(X_test_tfidf)

cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(4,3))
sns.heatmap(cm, annot=True, cmap='Blues', fmt='d')
plt.title("Confusion Matrix - Logistic Regression")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()
